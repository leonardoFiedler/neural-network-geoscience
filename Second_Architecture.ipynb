{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Architecture\n",
    "\n",
    "This is a simple perceptron architecture with only one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DEPTH_MD</th>\n      <th>X_LOC</th>\n      <th>Y_LOC</th>\n      <th>Z_LOC</th>\n      <th>CALI</th>\n      <th>RSHA</th>\n      <th>RMED</th>\n      <th>RDEP</th>\n      <th>RHOB</th>\n      <th>GR</th>\n      <th>...</th>\n      <th>Carbon_Index</th>\n      <th>Normalized_RHOB</th>\n      <th>Normalized_GR</th>\n      <th>Delta_DTC</th>\n      <th>Delta_RHOB</th>\n      <th>Delta_GR</th>\n      <th>Delta_DEPTH_MD</th>\n      <th>Delta_Carbon_Index</th>\n      <th>GROUP_encoded</th>\n      <th>FORMATION_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>494.528</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-469.501831</td>\n      <td>19.480835</td>\n      <td>-999.0</td>\n      <td>1.611410</td>\n      <td>1.798681</td>\n      <td>1.884186</td>\n      <td>80.200851</td>\n      <td>...</td>\n      <td>24.735691</td>\n      <td>0.314847</td>\n      <td>0.150172</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>6</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>494.680</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-469.653809</td>\n      <td>19.468800</td>\n      <td>-999.0</td>\n      <td>1.618070</td>\n      <td>1.795641</td>\n      <td>1.889794</td>\n      <td>79.262886</td>\n      <td>...</td>\n      <td>24.492376</td>\n      <td>0.318528</td>\n      <td>0.148269</td>\n      <td>0.527710</td>\n      <td>-0.005608</td>\n      <td>0.937965</td>\n      <td>0.152</td>\n      <td>-0.243315</td>\n      <td>6</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>494.832</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-469.805786</td>\n      <td>19.468800</td>\n      <td>-999.0</td>\n      <td>1.626459</td>\n      <td>1.800733</td>\n      <td>1.896523</td>\n      <td>74.821999</td>\n      <td>...</td>\n      <td>24.202299</td>\n      <td>0.322946</td>\n      <td>0.139258</td>\n      <td>0.429855</td>\n      <td>-0.006729</td>\n      <td>4.440887</td>\n      <td>0.152</td>\n      <td>-0.290077</td>\n      <td>6</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>494.984</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-469.957794</td>\n      <td>19.459282</td>\n      <td>-999.0</td>\n      <td>1.621594</td>\n      <td>1.801517</td>\n      <td>1.891913</td>\n      <td>72.878922</td>\n      <td>...</td>\n      <td>24.400797</td>\n      <td>0.319919</td>\n      <td>0.135315</td>\n      <td>0.024185</td>\n      <td>0.004610</td>\n      <td>1.943077</td>\n      <td>0.152</td>\n      <td>0.198498</td>\n      <td>6</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>495.136</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-470.109772</td>\n      <td>19.453100</td>\n      <td>-999.0</td>\n      <td>1.602679</td>\n      <td>1.795299</td>\n      <td>1.880034</td>\n      <td>71.729141</td>\n      <td>...</td>\n      <td>24.916765</td>\n      <td>0.312121</td>\n      <td>0.132982</td>\n      <td>0.021088</td>\n      <td>0.011879</td>\n      <td>1.149780</td>\n      <td>0.152</td>\n      <td>0.515968</td>\n      <td>6</td>\n      <td>68</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>",
      "text/plain": "   DEPTH_MD         X_LOC      Y_LOC       Z_LOC       CALI   RSHA      RMED  \\\n0   494.528  437641.96875  6470972.5 -469.501831  19.480835 -999.0  1.611410   \n1   494.680  437641.96875  6470972.5 -469.653809  19.468800 -999.0  1.618070   \n2   494.832  437641.96875  6470972.5 -469.805786  19.468800 -999.0  1.626459   \n3   494.984  437641.96875  6470972.5 -469.957794  19.459282 -999.0  1.621594   \n4   495.136  437641.96875  6470972.5 -470.109772  19.453100 -999.0  1.602679   \n\n       RDEP      RHOB         GR  ...  Carbon_Index  Normalized_RHOB  \\\n0  1.798681  1.884186  80.200851  ...     24.735691         0.314847   \n1  1.795641  1.889794  79.262886  ...     24.492376         0.318528   \n2  1.800733  1.896523  74.821999  ...     24.202299         0.322946   \n3  1.801517  1.891913  72.878922  ...     24.400797         0.319919   \n4  1.795299  1.880034  71.729141  ...     24.916765         0.312121   \n\n   Normalized_GR  Delta_DTC  Delta_RHOB  Delta_GR  Delta_DEPTH_MD  \\\n0       0.150172  -0.000000   -0.000000 -0.000000           0.000   \n1       0.148269   0.527710   -0.005608  0.937965           0.152   \n2       0.139258   0.429855   -0.006729  4.440887           0.152   \n3       0.135315   0.024185    0.004610  1.943077           0.152   \n4       0.132982   0.021088    0.011879  1.149780           0.152   \n\n   Delta_Carbon_Index  GROUP_encoded  FORMATION_encoded  \n0            0.000000              6                 68  \n1           -0.243315              6                 68  \n2           -0.290077              6                 68  \n3            0.198498              6                 68  \n4            0.515968              6                 68  \n\n[5 rows x 31 columns]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('data/raw/lithology.csv', sep=';')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c3e4cf18481f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Category Not Found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFORCE_2020_LITHOFACIES_LITHOLOGY_CAT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFORCE_2020_LITHOFACIES_LITHOLOGY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlithology_number_to_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "def lithology_number_to_name(x):\n",
    "    if x == 30000.0:\n",
    "        return 'Sandstone'\n",
    "    elif x == 65030.0:\n",
    "        return 'Sandstone/Shale'\n",
    "    elif x == 65000.0:\n",
    "        return 'Shale'\n",
    "    elif x == 80000.0:\n",
    "        return 'Marl'\n",
    "    elif x == 74000.0:\n",
    "        return 'Dolomite'\n",
    "    elif x == 70000.0:\n",
    "        return 'Limestone'\n",
    "    elif x == 70032.0:\n",
    "        return 'Chalk'\n",
    "    elif x == 88000.0:\n",
    "        return 'Halite'\n",
    "    elif x == 86000.0:\n",
    "        return 'Anhydrite'\n",
    "    elif x == 99000.0:\n",
    "        return 'Tuff'\n",
    "    elif x == 90000.0:\n",
    "        return 'Coal'\n",
    "    elif x == 93000.0:\n",
    "        return 'Basement'\n",
    "    else: raise ValueError('Category Not Found')\n",
    "\n",
    "df_train = df_train.assign(FORCE_2020_LITHOFACIES_LITHOLOGY_CAT = df_train.FORCE_2020_LITHOFACIES_LITHOLOGY.apply(lambda x: lithology_number_to_name(x)))\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Shale              720803\nSandstone          168937\nSandstone/Shale    150455\nLimestone           56320\nMarl                33329\nTuff                15245\nChalk               10513\nHalite               8213\nCoal                 3820\nDolomite             1688\nAnhydrite            1085\nBasement              103\nName: FORCE_2020_LITHOFACIES_LITHOLOGY_CAT, dtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_values_from_category(df_train, n_samples=1000):\n",
    "    '''\n",
    "    Get the same values (or all if it has less than) of all categories. The idea is to balance dataset\n",
    "    '''\n",
    "\n",
    "    df_data = pd.DataFrame()\n",
    "\n",
    "    for cat in df_train.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT.unique():\n",
    "        df_data = df_data.append(df_train[df_train.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT == cat].iloc[0:n_samples])\n",
    "    \n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "\n",
    "def transform_data(X_train, X_test, X_val, y_train, y_test, y_val):\n",
    "    scl = MinMaxScaler()\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    X_train_transform = scl.fit_transform(X_train)\n",
    "    X_test_transform = scl.transform(X_test)\n",
    "    X_val_transform = scl.transform(X_val)\n",
    "\n",
    "    y_train_enc = oe.fit_transform(y_train)\n",
    "    y_test_enc = oe.transform(y_test)\n",
    "    y_val_enc = oe.transform(y_val)\n",
    "\n",
    "    return X_train_transform, X_test_transform, X_val_transform, y_train_enc, y_test_enc, y_val_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_distribution(obj):\n",
    "    count_dict = {\n",
    "        'Anhydrite': 0, \n",
    "        'Basement': 0, \n",
    "        'Chalk': 0, \n",
    "        'Coal': 0, \n",
    "        'Dolomite': 0, \n",
    "        'Halite': 0,\n",
    "        'Limestone': 0, \n",
    "        'Marl': 0, \n",
    "        'Sandstone': 0, \n",
    "        'Sandstone/Shale': 0, \n",
    "        'Shale': 0,\n",
    "        'Tuff': 0\n",
    "    }\n",
    "    \n",
    "    for i in obj:\n",
    "        if i == 0: \n",
    "            count_dict['Anhydrite'] += 1\n",
    "        elif i == 1: \n",
    "            count_dict['Basement'] += 1\n",
    "        elif i == 2: \n",
    "            count_dict['Chalk'] += 1\n",
    "        elif i == 3: \n",
    "            count_dict['Coal'] += 1\n",
    "        elif i == 4: \n",
    "            count_dict['Dolomite'] += 1  \n",
    "        elif i == 5: \n",
    "            count_dict['Halite'] += 1\n",
    "        elif i == 6: \n",
    "            count_dict['Limestone'] += 1                    \n",
    "        elif i == 7: \n",
    "            count_dict['Marl'] += 1\n",
    "        elif i == 8: \n",
    "            count_dict['Sandstone'] += 1\n",
    "        elif i == 9: \n",
    "            count_dict['Sandstone/Shale'] += 1\n",
    "        elif i == 10: \n",
    "            count_dict['Shale'] += 1\n",
    "        elif i == 11: \n",
    "            count_dict['Tuff'] += 1\n",
    "        else:\n",
    "            print(\"Check classes.\")\n",
    "            \n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures To Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single_layer\n",
    "class SingleLayerSoftmax(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(SingleLayerSoftmax, self).__init__()\n",
    "        self.Linear = nn.Linear(n_input, n_output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.Linear(x)\n",
    "\n",
    "# multi_layer\n",
    "class MultiLayerSoftmax(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(MultiLayerSoftmax, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_input, 128)\n",
    "        self.layer_2 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "# multi_layer_relu\n",
    "class MultiLayerReLUSoftmax(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(MultiLayerReLUSoftmax, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_input, 128)\n",
    "        self.layer_2 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, n_output)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "# multi_layer_relu_batch_norm\n",
    "class MultiLayerReLUBatchNormSoftmax(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(MultiLayerReLUBatchNormSoftmax, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_input, 128)\n",
    "        self.layer_2 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, n_output)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "# multi_layer_relu_batch_norm_dropout\n",
    "class MultiLayerReLUBatchNormDropoutSoftmax(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(MultiLayerReLUBatchNormDropoutSoftmax, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_input, 128)\n",
    "        self.layer_2 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, n_output)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "    \n",
    "# multi_layer_3_relu_batch_norm_dropout\n",
    "class MultiLayer3ReLUBatchNormDropoutSoftmax(nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super(MultiLayer3ReLUBatchNormDropoutSoftmax, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_input, 512)\n",
    "        self.layer_2 = nn.Linear(512, 128)\n",
    "        self.layer_3 = nn.Linear(128, 64)\n",
    "        self.layer_out = nn.Linear(64, n_output)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm_3 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_3(x)\n",
    "        x = self.batch_norm_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.layer_out(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, num_features, num_classes):\n",
    "    if model_name == 'single_layer':\n",
    "        return SingleLayerSoftmax(num_features, num_classes)\n",
    "    elif model_name == 'multi_layer':\n",
    "        return MultiLayerSoftmax(num_features, num_classes)\n",
    "    elif model_name == 'multi_layer_relu':\n",
    "        return MultiLayerReLUSoftmax(num_features, num_classes)\n",
    "    elif model_name == 'multi_layer_relu_batch_norm':\n",
    "        return MultiLayerReLUBatchNormSoftmax(num_features, num_classes)\n",
    "    elif model_name == 'multi_layer_relu_batch_norm_dropout':\n",
    "        return MultiLayerReLUBatchNormDropoutSoftmax(num_features, num_classes)\n",
    "    elif model_name == 'multi_layer_3_relu_batch_norm_dropout':\n",
    "        return MultiLayer3ReLUBatchNormDropoutSoftmax(num_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a simple model\n",
    "\n",
    "Model created thanks to this documentation: https://towardsdatascience.com/pytorch-tabular-multiclass-classification-9f8211a123ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "def fit_model(\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val, \n",
    "    epochs=5, batch_size=16, num_classes=12, learning_rate=0.05,\n",
    "    model_name='single_layer'\n",
    "):\n",
    "    NUM_FEATURES = len(X.columns)\n",
    "    NUM_CLASSES = 12\n",
    "\n",
    "    train_dataset = ClassifierDataset(\n",
    "        torch.from_numpy(X_train.astype(np.float32)), \n",
    "        torch.from_numpy(y_train.reshape(-1)).long()\n",
    "    )\n",
    "\n",
    "    val_dataset = ClassifierDataset(\n",
    "        torch.from_numpy(X_val.astype(np.float32)), \n",
    "        torch.from_numpy(y_val.reshape(-1)).long()\n",
    "    )\n",
    "\n",
    "    test_dataset = ClassifierDataset(\n",
    "        torch.from_numpy(X_test.astype(np.float32)), \n",
    "        torch.from_numpy(y_test.reshape(-1)).long()\n",
    "    )\n",
    "\n",
    "    target_list = []\n",
    "    for _, t in train_dataset:\n",
    "        target_list.append(t)\n",
    "\n",
    "    target_list = torch.tensor(target_list)\n",
    "    target_list = target_list[torch.randperm(len(target_list))]\n",
    "\n",
    "    # Due to unbalanced data, create weights to classes\n",
    "    class_count = [i for i in get_class_distribution(y_train).values()]\n",
    "    class_weights = 1./torch.tensor(class_count, dtype=torch.float)\n",
    "\n",
    "    class_weights_all = class_weights[target_list]\n",
    "    weighted_sampler = WeightedRandomSampler(\n",
    "        weights=class_weights_all,\n",
    "        num_samples=len(class_weights_all),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=weighted_sampler\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=1)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=1)\n",
    "\n",
    "    model = load_model(model_name, NUM_FEATURES, NUM_CLASSES)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    accuracy_stats = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    loss_stats = {\n",
    "        'train': [],\n",
    "        'val': []\n",
    "    }\n",
    "\n",
    "    print('Begin training')\n",
    "\n",
    "    # Using tqdm to show the loading\n",
    "    for e in tqdm(range(1, epochs+1)):\n",
    "        # Training\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        for X_train_batch, y_train_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            train_loss = criterion(y_train_pred, y_train_batch)\n",
    "            train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_epoch_loss += train_loss.item()\n",
    "            train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        # Validation\n",
    "        with torch.no_grad():\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "        \n",
    "        train_loss = train_epoch_loss/len(train_loader)\n",
    "        val_loss = val_epoch_loss/len(val_loader)\n",
    "\n",
    "        train_acc = train_epoch_acc/len(train_loader)\n",
    "        val_acc = val_epoch_acc/len(val_loader)\n",
    "\n",
    "        loss_stats['train'].append(train_loss)\n",
    "        loss_stats['val'].append(val_loss)\n",
    "        accuracy_stats['train'].append(train_acc)\n",
    "        accuracy_stats['val'].append(val_acc)\n",
    "        print(f'''Epoch {e+0:03}: | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} | \n",
    "        Train Acc: {train_acc:.3f}% | Val Acc: {val_acc:.3f}%''')\n",
    "\n",
    "    train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\": \"epochs\"})\n",
    "\n",
    "    train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\": \"epochs\"})\n",
    "\n",
    "    # Plotting Accuracy and Loss\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
    "    sns.lineplot(data=train_val_acc_df, x='epochs', y='value', hue='variable', ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
    "    sns.lineplot(data=train_val_loss_df, x='epochs', y='value', hue='variable', ax=axes[1]).set_title('Train-Val Loss/Epoch')\n",
    "\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X_batch, _ in test_loader:\n",
    "            y_test_pred = model(X_batch)\n",
    "            _, y_pred_tags = torch.max(y_test_pred, dim=1)\n",
    "            y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    idx2class = {\n",
    "        0: 'Anhydrite', \n",
    "        1: 'Basement', \n",
    "        2: 'Chalk', \n",
    "        3: 'Coal', \n",
    "        4: 'Dolomite', \n",
    "        5: 'Halite',\n",
    "        6: 'Limestone', \n",
    "        7: 'Marl', \n",
    "        8: 'Sandstone', \n",
    "        9: 'Sandstone/Shale', \n",
    "        10: 'Shale',\n",
    "        11: 'Tuff'\n",
    "    }\n",
    "\n",
    "    # Confusion Matrix\n",
    "    # ax = plt.subplot()\n",
    "    plt.figure()\n",
    "    confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred_list)).rename(columns=idx2class, index=idx2class)\n",
    "    # sns.heatmap(confusion_matrix_df, annot=True, fmt='g', ax=ax)\n",
    "    sns.heatmap(confusion_matrix_df, annot=True, fmt='g')\n",
    "\n",
    "    print(classification_report(y_test, y_pred_list))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marl         33329\n",
      "Tuff         15245\n",
      "Chalk        10513\n",
      "Halite        8213\n",
      "Coal          3820\n",
      "Dolomite      1688\n",
      "Anhydrite     1085\n",
      "Basement       103\n",
      "Name: FORCE_2020_LITHOFACIES_LITHOLOGY_CAT, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "\n",
    "\n",
    "df_partial_data = df_train.query(\"FORCE_2020_LITHOFACIES_LITHOLOGY_CAT in ['Marl', 'Tuff', 'Chalk', 'Halite', 'Coal', 'Dolomite', 'Anhydrite', 'Basement']\")\n",
    "print(df_partial_data.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT.value_counts())\n",
    "\n",
    "X = df_partial_data[['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
    "    'RHOB', 'GR', 'NPHI', 'PEF','DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
    "    'MUDWEIGHT', 'RMIC', 'Carbon_Index',\n",
    "    'Normalized_RHOB', 'Normalized_GR', 'Delta_DTC', 'Delta_RHOB',\n",
    "    'Delta_GR', 'Delta_DEPTH_MD', 'Delta_Carbon_Index', 'GROUP_encoded',\n",
    "    'FORMATION_encoded']]\n",
    "Y = df_partial_data[['FORCE_2020_LITHOFACIES_LITHOLOGY_CAT']]\n",
    "\n",
    "X_balanced, y_balanced = oversample.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced_data = df_train.query(\"FORCE_2020_LITHOFACIES_LITHOLOGY_CAT not in ['Marl', 'Tuff', 'Chalk', 'Halite', 'Coal', 'Dolomite', 'Anhydrite', 'Basement']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_balanced_data.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT.value_counts()\n",
    "\n",
    "X1 = df_balanced_data[['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
    "    'RHOB', 'GR', 'NPHI', 'PEF','DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
    "    'MUDWEIGHT', 'RMIC', 'Carbon_Index',\n",
    "    'Normalized_RHOB', 'Normalized_GR', 'Delta_DTC', 'Delta_RHOB',\n",
    "    'Delta_GR', 'Delta_DEPTH_MD', 'Delta_Carbon_Index', 'GROUP_encoded',\n",
    "    'FORMATION_encoded']]\n",
    "Y1 = df_balanced_data[['FORCE_2020_LITHOFACIES_LITHOLOGY_CAT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced = X_balanced.append(X1)\n",
    "y_balanced = y_balanced.append(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_balanced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3780223c2456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFORCE_2020_LITHOFACIES_LITHOLOGY_CAT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_balanced' is not defined"
     ]
    }
   ],
   "source": [
    "y_balanced.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced.to_csv('data/raw/X_sub_augmented.csv')\n",
    "y_balanced.to_csv('data/raw/y_sub_augmented.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo-fiedler/Development/neural-network-geoscience/.venv/lib/python3.9/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "X_balanced = pd.read_csv('data/raw/X_sub_augmented.csv', index_col='Unnamed: 0')\n",
    "y_balanced = pd.read_csv('data/raw/y_sub_augmented.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Shale              720803\nSandstone          168937\nSandstone/Shale    150455\nLimestone           56320\nHalite              33329\nCoal                33329\nAnhydrite           33329\nDolomite            33329\nBasement            33329\nTuff                33329\nChalk               33329\nMarl                33329\nName: FORCE_2020_LITHOFACIES_LITHOLOGY_CAT, dtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_balanced.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DEPTH_MD</th>\n      <th>X_LOC</th>\n      <th>Y_LOC</th>\n      <th>Z_LOC</th>\n      <th>CALI</th>\n      <th>RSHA</th>\n      <th>RMED</th>\n      <th>RDEP</th>\n      <th>RHOB</th>\n      <th>GR</th>\n      <th>...</th>\n      <th>Normalized_RHOB</th>\n      <th>Normalized_GR</th>\n      <th>Delta_DTC</th>\n      <th>Delta_RHOB</th>\n      <th>Delta_GR</th>\n      <th>Delta_DEPTH_MD</th>\n      <th>Delta_Carbon_Index</th>\n      <th>GROUP_encoded</th>\n      <th>FORMATION_encoded</th>\n      <th>FORCE_2020_LITHOFACIES_LITHOLOGY_CAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2198.296</td>\n      <td>437622.28125</td>\n      <td>6470981.0</td>\n      <td>-2172.966553</td>\n      <td>23.249596</td>\n      <td>-999.0</td>\n      <td>0.474248</td>\n      <td>0.447025</td>\n      <td>1.677679</td>\n      <td>48.699841</td>\n      <td>...</td>\n      <td>0.179283</td>\n      <td>0.086253</td>\n      <td>-0.002747</td>\n      <td>0.001873</td>\n      <td>-1.837376</td>\n      <td>0.152</td>\n      <td>0.102723</td>\n      <td>7</td>\n      <td>4</td>\n      <td>Tuff</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>2198.296</td>\n      <td>437622.28125</td>\n      <td>6470981.0</td>\n      <td>-2172.966553</td>\n      <td>23.249596</td>\n      <td>-999.0</td>\n      <td>0.474248</td>\n      <td>0.447025</td>\n      <td>1.677679</td>\n      <td>48.699841</td>\n      <td>...</td>\n      <td>0.179283</td>\n      <td>0.086253</td>\n      <td>-0.002747</td>\n      <td>0.001873</td>\n      <td>-1.837376</td>\n      <td>0.152</td>\n      <td>0.102723</td>\n      <td>7</td>\n      <td>4</td>\n      <td>Shale</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>494.528</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-469.501831</td>\n      <td>19.480835</td>\n      <td>-999.0</td>\n      <td>1.611410</td>\n      <td>1.798681</td>\n      <td>1.884186</td>\n      <td>80.200851</td>\n      <td>...</td>\n      <td>0.314847</td>\n      <td>0.150172</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>6</td>\n      <td>68</td>\n      <td>Tuff</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>494.528</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-469.501831</td>\n      <td>19.480835</td>\n      <td>-999.0</td>\n      <td>1.611410</td>\n      <td>1.798681</td>\n      <td>1.884186</td>\n      <td>80.200851</td>\n      <td>...</td>\n      <td>0.314847</td>\n      <td>0.150172</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>6</td>\n      <td>68</td>\n      <td>Shale</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>494.528</td>\n      <td>437641.96875</td>\n      <td>6470972.5</td>\n      <td>-469.501831</td>\n      <td>19.480835</td>\n      <td>-999.0</td>\n      <td>1.611410</td>\n      <td>1.798681</td>\n      <td>1.884186</td>\n      <td>80.200851</td>\n      <td>...</td>\n      <td>0.314847</td>\n      <td>0.150172</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>-0.000000</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>6</td>\n      <td>68</td>\n      <td>Tuff</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>",
      "text/plain": "   DEPTH_MD         X_LOC      Y_LOC        Z_LOC       CALI   RSHA      RMED  \\\n0  2198.296  437622.28125  6470981.0 -2172.966553  23.249596 -999.0  0.474248   \n0  2198.296  437622.28125  6470981.0 -2172.966553  23.249596 -999.0  0.474248   \n0   494.528  437641.96875  6470972.5  -469.501831  19.480835 -999.0  1.611410   \n0   494.528  437641.96875  6470972.5  -469.501831  19.480835 -999.0  1.611410   \n0   494.528  437641.96875  6470972.5  -469.501831  19.480835 -999.0  1.611410   \n\n       RDEP      RHOB         GR  ...  Normalized_RHOB  Normalized_GR  \\\n0  0.447025  1.677679  48.699841  ...         0.179283       0.086253   \n0  0.447025  1.677679  48.699841  ...         0.179283       0.086253   \n0  1.798681  1.884186  80.200851  ...         0.314847       0.150172   \n0  1.798681  1.884186  80.200851  ...         0.314847       0.150172   \n0  1.798681  1.884186  80.200851  ...         0.314847       0.150172   \n\n   Delta_DTC  Delta_RHOB  Delta_GR  Delta_DEPTH_MD  Delta_Carbon_Index  \\\n0  -0.002747    0.001873 -1.837376           0.152            0.102723   \n0  -0.002747    0.001873 -1.837376           0.152            0.102723   \n0  -0.000000   -0.000000 -0.000000           0.000            0.000000   \n0  -0.000000   -0.000000 -0.000000           0.000            0.000000   \n0  -0.000000   -0.000000 -0.000000           0.000            0.000000   \n\n   GROUP_encoded  FORMATION_encoded  FORCE_2020_LITHOFACIES_LITHOLOGY_CAT  \n0              7                  4                                  Tuff  \n0              7                  4                                 Shale  \n0              6                 68                                  Tuff  \n0              6                 68                                 Shale  \n0              6                 68                                  Tuff  \n\n[5 rows x 31 columns]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_balanced = X_balanced.join(y_balanced)\n",
    "df_data_balanced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with all data\n",
    "df_partial_data = get_values_from_category(df_data_balanced, n_samples=1000)\n",
    "print(df_partial_data.FORCE_2020_LITHOFACIES_LITHOLOGY_CAT.value_counts())\n",
    "\n",
    "X = df_partial_data[['DEPTH_MD', 'X_LOC', 'Y_LOC', 'Z_LOC', 'CALI', 'RSHA', 'RMED', 'RDEP',\n",
    "    'RHOB', 'GR', 'NPHI', 'PEF','DTC', 'SP', 'BS', 'ROP', 'DCAL', 'DRHO',\n",
    "    'MUDWEIGHT', 'RMIC', 'Carbon_Index',\n",
    "    'Normalized_RHOB', 'Normalized_GR', 'Delta_DTC', 'Delta_RHOB',\n",
    "    'Delta_GR', 'Delta_DEPTH_MD', 'Delta_Carbon_Index', 'GROUP_encoded',\n",
    "    'FORMATION_encoded']]\n",
    "Y = df_partial_data[['FORCE_2020_LITHOFACIES_LITHOLOGY_CAT']]\n",
    "\n",
    "#Dataset division: 60:20:20\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, Y,\n",
    "                                                    stratify=Y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,\n",
    "                                                    stratify=y_train_val, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train_transform, X_test_transform, X_val_transform, y_train_transform, y_test_transform, y_val_transform = transform_data(X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "\n",
    "model = fit_model(\n",
    "    X_train_transform, y_train_transform, X_test_transform, y_test_transform, X_val_transform, y_val_transform, \n",
    "    epochs=2, batch_size=32, num_classes=12, learning_rate=0.01,\n",
    "    model_name='multi_layer_3_relu_batch_norm_dropout'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "f57ac3dc69f9b6a7ca8341da2f49afa2cd89ab2c0cc6c1a8b4fb989970585386"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}